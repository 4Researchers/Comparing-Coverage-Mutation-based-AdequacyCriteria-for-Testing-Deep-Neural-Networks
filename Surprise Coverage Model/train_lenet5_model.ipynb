{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input,Add,Conv2D,Dense,ZeroPadding2D,Activation,MaxPooling2D,Flatten\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist, cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "CLIP_MIN = -0.5\n",
    "CLIP_MAX = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'sadl-master'\n",
      "/home/ict520c/notebooks/sadl-master\n"
     ]
    }
   ],
   "source": [
    "cd sadl-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 16:06:20.303728 140465949632320 module_wrapper.py:137] From /home/ict520c/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0912 16:06:20.318116 140465949632320 module_wrapper.py:137] From /home/ict520c/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0912 16:06:20.387714 140465949632320 module_wrapper.py:137] From /home/ict520c/.local/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0912 16:06:20.462375 140465949632320 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3258 - acc: 0.9050 - val_loss: 0.0960 - val_acc: 0.9703\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0916 - acc: 0.9718 - val_loss: 0.0710 - val_acc: 0.9783\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.0651 - acc: 0.9804 - val_loss: 0.0544 - val_acc: 0.9823\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0528 - acc: 0.9842 - val_loss: 0.0429 - val_acc: 0.9865\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0432 - acc: 0.9864 - val_loss: 0.0485 - val_acc: 0.9834\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0388 - acc: 0.9875 - val_loss: 0.0411 - val_acc: 0.9862\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.0316 - acc: 0.9904 - val_loss: 0.0455 - val_acc: 0.9870\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0285 - acc: 0.9907 - val_loss: 0.0510 - val_acc: 0.9846\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0260 - acc: 0.9915 - val_loss: 0.0375 - val_acc: 0.9878\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0230 - acc: 0.9929 - val_loss: 0.0385 - val_acc: 0.9877\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.0219 - acc: 0.9928 - val_loss: 0.0390 - val_acc: 0.9881\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0387 - val_acc: 0.9873\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0153 - acc: 0.9950 - val_loss: 0.0359 - val_acc: 0.9899\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0145 - acc: 0.9950 - val_loss: 0.0382 - val_acc: 0.9891\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0132 - acc: 0.9957 - val_loss: 0.0416 - val_acc: 0.9881\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0142 - acc: 0.9951 - val_loss: 0.0447 - val_acc: 0.9876\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 158us/step - loss: 0.0105 - acc: 0.9965 - val_loss: 0.0438 - val_acc: 0.9886\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0371 - val_acc: 0.9915\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0371 - val_acc: 0.9902\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0380 - val_acc: 0.9896\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Test Loss: 0.03803748108773866\n",
      "Test accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "path = 'original_data/'\n",
    "files = ['train-images-idx3-ubyte.gz','train-labels-idx1-ubyte.gz'\n",
    ",'t10k-images-idx3-ubyte.gz','t10k-labels-idx1-ubyte.gz']\n",
    "paths = [path+each for each in files]\n",
    "with gzip.open(paths[1],'rb') as imgpath:\n",
    "    y_train = np.frombuffer(imgpath.read(),np.uint8,offset=8)\n",
    "with gzip.open(paths[0],'rb') as imgpath:\n",
    "    x_train = np.frombuffer(imgpath.read(),np.uint8,offset=16).reshape(len(y_train),28,28,1)\n",
    "with gzip.open(paths[3],'rb') as imgpath:\n",
    "    y_test= np.frombuffer(imgpath.read(),np.uint8,offset=8)\n",
    "with gzip.open(paths[2],'rb') as imgpath:\n",
    "    x_test = np.frombuffer(imgpath.read(),np.uint8,offset=16).reshape(len(y_test),28,28,1)\n",
    "        \n",
    "\n",
    "\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_test = (x_test / 255.0) - (1.0 - 0.5)\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_train= x_train.astype(\"float32\")\n",
    "x_train= (x_train / 255.0) - (1.0 - 0.5)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 选取6个特征卷积核，大小为5∗5(不包含偏置),得到66个特征图，每个特征图的大小为32−5+1=2832−5+1=28，\n",
    "# 也就是神经元的个数由10241024减小到了28∗28=78428∗28=784。\n",
    "# 输入层与C1层之间的参数:6∗(5∗5+1)\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "\n",
    "# 这一层的输入为第一层的输出，是一个28*28*6的节点矩阵。\n",
    "# 本层采用的过滤器大小为2*2，长和宽的步长均为2，所以本层的输出矩阵大小为14*14*6。\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 本层的输入矩阵大小为14*14*6，使用的过滤器大小为5*5，深度为16.本层不使用全0填充，步长为1。\n",
    "# 本层的输出矩阵大小为10*10*16。本层有5*5*6*16+16=2416个参数\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "\n",
    "# 本层的输入矩阵大小10*10*16。本层采用的过滤器大小为2*2，长和宽的步长均为2，所以本层的输出矩阵大小为5*5*16。\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 本层的输入矩阵大小为5*5*16，在LeNet-5论文中将这一层称为卷积层，但是因为过滤器的大小就是5*5，#\n",
    "# 所以和全连接层没有区别。如果将5*5*16矩阵中的节点拉成一个向量，那么这一层和全连接层就一样了。\n",
    "# 本层的输出节点个数为120，总共有5*5*16*120+120=48120个参数。\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation='relu'))\n",
    "\n",
    "# 本层的输入节点个数为120个，输出节点个数为84个，总共参数为120*84+84=10164个 (w + b)\n",
    "model.add(Dense(84, activation='relu'))\n",
    "\n",
    "# 本层的输入节点个数为84个，输出节点个数为10个，总共参数为84*10+10=850\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "   model.save(\"./model/lenet_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-92deedea1fc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: model"
     ]
    }
   ],
   "source": [
    "print([layer.name for layer in model.get_layer('model').layers])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Image.open('orig.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
